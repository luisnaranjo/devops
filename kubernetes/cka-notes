----------------------------------------------- INTRODUCTION -----------------------------------------------
[K8 BASICS]
Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.
The name Kubernetes originates from Greek, meaning helmsman or pilot. Google open-sourced the Kubernetes project in 2014.
K8s is simply short for Kubernetes. The 8 represents the 8 letters between the K and the S.

Kubernetes builds a layer of abstraction for servers running containers. This is known as a K8s cluster.
Multiple servers make up the K8 cluster.


[KUBERNETES FEATURES]
CONTAINER ORCHESTRATION:
The primary purpose of Kubernetes is to dynamically manage containers across multiple host systems.

APPLICATION RELIABILITY:
Kubernetes makes it easier to build reliable self-healing, and scalable applications.

AUTOMATION:
Kubernetes offers a variety of features to help automate the management of your container apps.


[K8S ARCHITECTURAL OVERVIEW]
CONTROL PLANE:
The control plane is a collection of multiple components responsible for managing the cluster itself globally.
Essentially, the control plane controls the cluster.
Individual control plane components can run on any machine in the cluster, but usually are run on dedicated controller machines.

    - kube-api-server:  It serves the Kubernetes API, the primary interface to the control plane and the cluster itself.
                        When interacting with your Kubernetes cluster, you will usually do so using the Kubernetes API.

    - etcd: It's the backend data store for the Kubernetes cluster. It provides high-availability storage for all data relating to the state of the cluster.

    - kube-scheduler: It handles scheduling, the process of selecting an available node in the cluster on which to run containers.

    - kube-controller-manager:  It runs a collection of multiple controller utilities in a single process.
                                These controllers carry out a variety of automation-related tasks within the Kubernetes cluster.

    - cloud-controller-manager: It provides an interface between Kubernetes and various cloud platforms.
                                It is only used when using cloud-based resources alongside Kubernetes.

KUBERNETES NODES:
Kubernetes nodes are the machines where the containers managed by the cluster run. A cluster can have any number of nodes.
Various nodes components manage containers on the machine and communicate with the control plane.

    - kubelet:  It's the Kubernetes agent that runs on each node. It communicates with the control plane and ensures that containers are run on its node as instructed by the control plane.
                Kubelet also handles the process of reporting container status and other data about containers back to the control plane.

    - Container runtime:    The container runtime is not built into Kubernetes. It's a separate piece of software that is responsible for actually running containers on the machine.
                            Kubernetes supports multiple container runtime implementations. Some popular container runtime are Docker and containerd.

    - kube-proxy: It's a network proxy. It runs on each node and handles some tasks related to providing networking between the containers and services in the cluster.


[BUILDING A K8S CLUSTER]
KUBEADM:
You can simplify the process of building a cluster by using "kubeadm" to configure your control plane and worker nodes.

For building this cluster, we are going to set a cluster of 3 nodes, one controller node, and two worker nodes.
We will use kubeadm utility to simplify the setup process.

CONTAINERD SETUP (All nodes):
On all of the nodes, set up containerd. Load the next kernel modules and create a conf file so these modules are loaded at every boot.
    sudo modprobe overlay       # Load the overlay kernel module.
    sudo modprobe br_netfilter  # Load the br_netfilter kernel module.

    vi /etc/modules-load.d/containerd.conf
        overlay
        br_netfilter

Create the next system setting conf file:
This settings are needed for Kubernetes networking.
    vi /etc/sysctl.d/99-kubernetes-cri.conf
        net.bridge.bridge-nf-call-iptables  = 1
        net.ipv4.ip_forward                 = 1
        net.bridge.bridge-nf-call-ip6tables = 1

Run the next command to apply these settings now:
    sudo sysctl --system

Install and configure containerd:
    sudo apt-get update && sudo apt-get install -y containerd               # Install containerd.
    sudo mkdir -p /etc/containerd                                           # Containerd config dir.
    sudo containerd config default | sudo tee /etc/containerd/config.toml   # Create a default containerd config and save it in file.
    sudo systemctl restart containerd                                       # Restart containerd runtime.

DISABLE SWAP (All nodes):
In order to Kubernetes to work, you would need to disable swap on all nodes:
    sudo swapoff -a                                     # Disable swap.
    sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab  # Ensure swap is disabled across reboots.

INSTALL K8S PACKAGES (All nodes):
On all nodes, install kubeadm, kubelet, kubectl:
    sudo apt-get update && sudo apt-get install -y apt-transport-https curl                 # Install dependencies.
    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -      # Add the GPG key.
    vi /etc/apt/sources.list.d/kubernetes.list                                              # Configure the Kubernetes repo.
        deb https://apt.kubernetes.io/ kubernetes-xenial main

    sudo apt-get update
    sudo apt-get install -y kubelet=1.20.1-00 kubeadm=1.20.1-00 kubectl=1.20.1-00           # Install K8s packages.
    #sudo apt-get install -y {kubelet,kubeadm,kubectl}=1.20.1-00                            # Can also be run this way.
    sudo apt-mark hold kubelet kubeadm kubectl                                              # Ensure K8s packages don't get upgraded.

INITIALIZE THE CLUSTER AND SET UP KUBECTL ACCESS (Controller node):
On the control plane node only, initialize the cluster and set up kubectl access:
    sudo kubeadm init --pod-network-cidr 192.168.0.0/16         # Initialize the cluster.
    mkdir -p $HOME/.kube                                        # Create the user-scoped kube config dir.
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config    # Place the admin config file into your K8s user dir.
    sudo chown $(id -u):$(id -g) $HOME/.kube/config             # Correct the file ownership.

Verify the cluster is working:
You should get a client version and server version.
    kubectl version

CONFIGURE THE CLUSTER NETWORKING SOLUTION (Controller node):
On the control plane node only, apply the calico plugin for the cluster:
    kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml   # Apply the yaml configs for the networking plugin.

Verify the calico pods are running:
    kubectl get pods -n kube-system     # Display all pods in the kube-system namespace.

ADD THE WORKER NODES TO THE CLUSTER (Worker nodes):
On the control plane node, run the next command to get the join token for worker nodes:
This command can also be found in the output of the "kubeadm init" command.
    kubectl token create --print-join-command   # Generate cluster join token.

Copy the join command from the control plane node, and run it on each of the worker nodes as root (or with sudo):
    sudo kubeadm join CONTROLLERIP:CONTROLLERPORT --token TOKEN --discovery-token-ca-cert-hash HASH

VERIFY ALL NODES IN THE CLUSTER ARE READY (Controller nodes):
On the control plane node, verify all nodes are ready. It may take a moment for all nodes to be in "Ready" status.
    kubectl get nodes   # Display the cluster nodes.


[NAMESPACES]
Namespaces are virtual clusters backed by the same physical cluster. Kubernetes objects, such as pods and containers, live in namespaces.
Namespaces are a way to separate and organize objects in your cluster.

Namespaces are intended for use in environments with many users spread across multiple teams, or projects.
Namespaces are a way to divide cluster resources between multiple users.

Namespaces provide scope for names. Names of resources need to be unique within a namespace, but not across namespaces.
Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.

You don't need to create multiple namespaces just to separate slightly different resources, such as different versions of the same software.
Use "labels" to distinguish resources within the same namespace.

All clusters have a "default" namespace. This is used when no other namespace is specified.
kubeadm command also creates a "kube-system" namespace for system components.

DISPLAY NAMESPACES:
You can list existing namespaces with kubectl command:
    kubectl get namespaces      # Display K8s namespaces.

When using kubectl, you may need to specify a namespace. You can do this with the "--namespace" or "-n" flag.
Note that if you don't specify a namespace, the default is assumed.
    kubectl get pods --namespace my-namespace   # Display all pods in "my-namespace" namespace.
    kubectl get pods --all-namespaces           # Display all the pods form all namespaces.

CREATE NAMESPACES:
You can create namespaces with kubectl command.
    kubectl create namespace my-namespace       # Creates a namespace with the specified name.



----------------------------------------------- CLUSTER MANAGEMENT -----------------------------------------------
[INTRODUCTION TO HA IN K8S]
K8s facilitates high-availability applications, but you can also design the cluster itself to be high available.
To do this, you would need multiple control plane nodes.

When using multiple control planes for high availability, you will likely need to communicate with the Kubernetes API through a load balancer.
This includes clients such as kubelet instances running on worker nodes.

With HA in a K8s cluster, you have several ways to manage etcd.

STACKED ETCD:
With Stacked etcd, etcd runs on the same servers/nodes as the rest of the control plane components.
This setup is used with clusters configured using kubeadm.
In this setup, each control plane node will have its own etcd instance.

EXTERNAL ETCD:
With stacked etcd, etcd is running on the same node as the control plane components.
With external etcd, you have etcd running on completely separate nodes.
With this setup, you can have any number of Kubernetes control plane instances, and any number of external nodes running their own etcd instance.


[K8S MANAGEMENT TOOLS]
There are a variety of management tools available for Kubernetes. These tools interface with Kubernetes to provide additional functionality.
When using Kubernetes, it's a good idea to be aware of some of these tools.

KUBECTL:
kubectl is the official command line interface for Kubernetes. It controls the Kubernetes cluster manager.

KUBEADM:
kubeadm is a tool for quickly and easily creating a secure Kubernetes clusters.

MINIKUBE:
Minikube allows you to automatically set up a local single-node Kubernetes cluster. It's great for getting Kubernetes up and running quickly for development purposes.

DASHBOARD:
Dashboard is the web-based interface of Kubernetes, allows you to deploy containerized applications to a Kubernetes cluster, troubleshoot them, and manage the cluster and its resources itself.

HELM:
Helm provides templating and package management for Kubernetes objects.
You can use it to manage your own templates (known as Charts). You can also download and use shared templates.

KOMPOSE:
Kompose helps you translate Docker compose files into Kubernetes objects.
If you are using Docker compose for some part of your workflow, you can move your application to Kubernetes easily with Kompose.

KUSTOMIZE:
Kustomize is a configuration management tool for managing Kubernetes object configurations.
It allows you to share and re-use templated configurations for Kubernetes applications.
The tool provides a new, purely declarative approach to configuration customization that adheres to and leverages the familiar and carefully designed Kubernetes API.
Every artifact that kustomize uses is plain YAML and can be validated and processed as such.


[SAFELY DRAINING A K8S NODE]
In order to perform maintenance on your Kubernetes nodes, you need to be able to transfer workloads to other nodes so maintenance tasks do not interfere with ongoing service.

DRAINING:
When performing maintenance, you may sometimes need to remove a Kubernetes node from service.
To do this, you can drain the node. Containers running on the node will be gracefully terminated (and potentially rescheduled on another node).

DRAINING A NODE:
To drain a node, use the "kubectl drain" command.
    kubectl drain NODENAME      # Drain the specified node form the cluster.

IGNORING DAEMONSETS:
When draining a node, you may need to ignore DaemonSets (pods that are tied to each node).
If you have any DaemonSets pods running on the node, you will likely need to use the "--ignore-daemonsets" flag.
    kubectl drain NODENAME --ignore-daemonsets      # Drain the specified node from the cluster ignoring any DaemonSet.

UNCORDONING A NODE:
If the node remains part of the cluster, you can allow pods to run on the node again when maintenance is complete using the "kubectl uncordon" command.
    kubectl uncordon NODENAME       # Uncordon the specified node to the cluster (re-active it to run containers again).

Uncordoning a node doesn't automatically re-balance deployment pods.


[UPGRADING K8S WITH KUBEADM]
When using Kubernetes, you will likely want to periodically upgrade Kubernetes to keep your cluster up to date. Kubeadm makes this process easier.
To perform the upgrade, you would need to upgrade the control plane nodes, as well as the worker nodes.

In a real-world scenario, you shouldn't perform upgrades on all worker nodes at the same time.
Make sure enough nodes are available at any given time to provide uninterrupted service.

CONTROL PLANE UPGRADE STEPS:
You would need to perform this step on every control plane node when upgrading your cluster.

Upgrade kubeadm on the control plane node. The flag "--allow-change-held-packages" is needed if you have the packaged marked as hold.
    sudo apt-get update && sudo apt-get install -y --allow-change-held-packages kubeadm=1.20.2-00   # Upgrade the held kubeadm version.
    kubeadm version                                                                                 # Display the kubeadm version.

Drain the control plane node. Make sure to specify the "--ignore-daemonsets" flag, since control plane nodes will have daemonsets running.
    kubectl drain CONTROLPLANENODE --ignore-daemonsets      # Drain the specified control plane node ignoring daemonsets.

Plan the upgrade (kubeadm upgrade plan).
    sudo kubeadm upgrade plan VERSION       # Plan the upgrade.
    sudo kubeadm upgrade plan v1.20.2       # Plan the upgrade to version 1.20.2
This plan will let you know what is going to be done as part of the upgrade process.

Apply the upgrade (kubeadm upgrade apply).
    sudo kubeadm upgrade apply VERSION      # Apply the upgrade.
    sudo kubeadm upgrade apply v1.20.2      # Apply the upgrade to version 1.20.2

Upgrade kubelet and kubectl on the control plane node.
    sudo apt-get update && sudo apt-get install -y --allow-change-held-packages {kubelet,kubectl}=1.20.2-00  # Upgrade the held kubectl & kubelet versions.

Restart kubelet on the control plane node.
    sudo systemctl daemon-reload    # Reload the unit files for the services.
    sudo systemctl restart kubelet  # Restart kubelet service.
It's possible that the upgrade modifies the unit file for kubelet. That's why "systemctl daemon-reload" command is needed.

Uncordon the control plane node.
    kubectl uncordon CONTROLPLANENODE       # Uncordon the control plane node specified.

Verify that the control plane is working.
    kubectl get nodes       # Display the cluster nodes.

WORKER NODE UPGRADE STEPS:
You would need to perform this step on every worker node when upgrading your cluster.

Drain the worker node from the cluster, by running the next in a control plane node.
    kubectl drain WORKERNODE --ignore-daemonsets --force
The "--force" flag is needed in case you have a single pod running on the node.
Before using this flag, make sure the pod can be removed without issues, since this will delete it from the cluster (it won't be replicated on another worker node).

Upgrade kubeadm. On the worker node, upgrade the kubeadm package.
    sudo apt-get update && sudo apt-get install -y --allow-change-held-packages kubeadm=1.20.2-00

Verify the kubeadm version.
    kubeadm version

Upgrade the kubelet configuration on the worker node via kubeadm.
    sudo kubeadm upgrade node   # Upgrade the kubelet configuration on the worker node.

Upgrade kubelet & kubectl versions on the worker node.
    sudo apt-get update && sudo apt-get install -y --allow-change-held-packages {kubelet,kubectl}=1.20.2-00

Restart kubelet on the worker node.
    sudo systemctl daemon-reload
    sudo systemctl restart kubelet

From the control plane node, uncordon the worker node.
    kubectl uncordon WORKERNODE


[BACKING UP AND RESTORING ETCD CLUSTER DATA]
All Kubernetes objects are stored in etcd. Periodically backing up the etcd cluster data is important to recover Kubernetes clusters under disaster scenarios, such as losing all control plane nodes.
The snapshot file contains all the Kubernetes states and critical information. In order to keep the sensible Kubernetes data safe, encrypt the snapshot files.

Backing up an etcd cluster can be accomplished in 2 ways:
    - etcd built-in snapshot.
    - Volume snapshot (outside of the etcd solution).

BACKING UP ETCD WITH BUILT-IN SNAPSHOTS:
etcd supports built-in snapshots.
A snapshot may either be taken from a live member with the "etcdctl snapshot save" command or by copying the "member/snap/db" file from an etcd data directory that is not currently used by an etcd process.
Taking the snapshot will not affect the performance of the member
    ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save SNAPSHOTNAME  # Takes an etcd snapshot with the name specified.
    ETCDCTL_API=3 etcdctl --write-out=table snapshot status SNAPSHOTNAME    # Verify the snapshot status.

BACKING UP ETCD WITH VOLUME SNAPSHOT:
If etcd is running on a storage volume that supports backup, such as AWS Elastic Block Storage, back up etcd data by taking a snapshot of the storage volume.

RESTORING ETCD:
etcd supports restoring from snapshots that are taken from an etcd process of the major.minor version.
Restoring a version from a different patch version of etcd is also supported. A restore operation is employed to recover data of a failed cluster.

Before starting the restore operation, a snapshot file must be present. It can be either a snapshot file from a previous backup operation, or from a remaining data directory.

If the majority of etcd members have permanently failed, the etcd cluster is considered failed.
In this scenario, Kubernetes cannot make any changes to its current state. Although the scheduled pods might continue to run, no new pods can be scheduled.
In such cases, recover the etcd cluster and potentially reconfigure Kubernetes API servers to fix the issue.

You can restore etcd data from backup using "etcdctl snapshot restore" command.
You will need to supply some additional parameters, as the restore operation creates a new temporally logical cluster in order to repopulate the data from the backup.
    ETCDCTL_API=3 etcdctl snapshot restore FILENAME     # Restore etcd data from given backup.

NOTE: If any API servers are running in your cluster, you shouldn't attempt to restore instances of etcd. Instead, follow the next steps to restore etcd:
    - Stop ALL API server instances.
    - Restore state in all etcd instances.
    - Restart all API server instances.
It's also recommended to restart any components (kube-scheduler, kube-controller-manager, kubelet) to ensure they don't rely on some stale data.



----------------------------------------------- KUBERNETES OBJECT MANAGEMENT -----------------------------------------------
[WORKING WITH KUBECTL]
The kubectl command line tool lets you control Kubernetes clusters. kubectl uses the Kubernetes API to communicate with the cluster and carry out your commands.
You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.

For configuration, kubectl looks for a filed named "config" in the $HOME/.kube directory.
You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag.

SYNTAX:
Use the following syntax to run kubectl commands from your terminal window:
    kubectl [COMMAND] [TYPE] [NAME] [FLAGS]
Where:
    - COMMAND: Specifies the operation that you want to perform on one or more resources (create, get, describe, delete).
    - TYPE: Specifies the "resource type". Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms.
    - NAME: Specifies the name of the resource. Names are case-sensitive. If name is omitted, details for all resources are displayed.
            When performing an operation on multiple resources, you can specify each resource by type and name or specify one or more files.
            To specify resources by type and name:
                * To group resources if they are all the same type: TYPE name1 name2 name#.
                    Example: kubectl get pod example-pod1 example-pod2
                * To specify multiple resource types individually: TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE#/name#
                    Example: kubectl get pod/example-pod1 replicationcontroller/example-rc1
            To specify resources with one or more files: -f file1 -f file2 -f file#:
                * Use YAML rather than JSON since YAML tends to be more user-friendly, especially for configuration files.
                    Example: kubectl get -f ./pod.yaml

    - FLAGS: Specifies optional flags. Flags that you specify from the command line override default values and any corresponding environment variables.

COMMAND KUBECTL GET:
Use kubectl get to list objects in the Kubernetes cluster.
Structure:
    kubectl get OBJECTTYPE OBJECTNAME [FLAGS]
Flags:
    - -o OUTPUT: Set output format.
    - --sort-by JSONPATH: Sort output using a JSONPath expression.
    - --selector SELECTOR: Filter results by label.
Examples:
    kubectl get pods                                                # Display pods in the default namespace.
    kubectl get po                                                  # Display pods in the default namespace.
    kubectl get pod my-pod -o wide                                  # Display pods matching the name with additional information.
    kubectl get pods -o json                                        # Display pods in the default namespace in JSON format.
    kubectl get pods -o yaml                                        # Display pods in the default namespace in YAML format.
    kubectl get pods -o wide --sort-by .spec.nodeName               # Sorts by node name all of the pods in the default namespace.
    kubectl get pods -n kube-system --selector k8s-app=calico-node  # Display only the pods in kube-system namespace with the specified label.

COMMAND KUBECTL DESCRIBE:
You can get detailed information about Kubernetes objects using kubectl describe.
Structure:
    kubectl describe OBJECTTYPE OBJECTNAME
Examples:
    kubectl describe pod my-pod     # Display in-depth information about the pod.

COMMAND KUBECTL CREATE:
Use kubectl create to create objects. If you attempt to create an object that already exists, an error will occur.
You can supply a YAML file with "-f" flag to create an object from a YAML descriptor stored in the file.
Structure:
    kubectl create -f FILENAME
Examples:
    kubectl create -f pod.yml       # Create the K8s object with the specifications in the file.

COMMAND KUBECTL APPLY:
kubectl apply is similar to kubectl create command. However, if you use kubectl apply on an object that already exists, it will modify the existing object, if possible.
Structure:
    kubectl apply -f FILENAME
Examples:
    kubectl apply -f pod.yml       # If not created, create the K8s object with the specifications in the file. If no change is needed, it won't perform anything.

COMMAND KUBECTL DELETE:
Use kubectl delete to delete objects from the cluster.
Structure:
    kubectl delete OBJECTTYPE OBJECTNAME
Examples:
    kubectl delete pod my-pod       # Delete the pod.

COMMAND KUBECTL EXEC:
kubectl exec can be used to run commands inside containers. Keep in mind that, in order for a command to succeed, the necessary software must exists within the container to run it.
For pods with multiple containers, specify the container name with "-c" flag.
Structure:
    kubectl exec PODNAME [FLAGS] -- COMMAND
Flags:
    - -c CONTAINERNAME: Specify the container name within the pod.
Example:
    kubectl exec my-pod -c busybox -- echo "Hello world!"       # Executes the command in the container inside the pod.


[KUBECTL TIPS]
IMPERATIVE COMMANDS:
When creating Kubernetes objects there are two ways to do so.
The first one and the mostly used is the Declarative commands. On this you define objects using data structures such as YAML or JSON and specify the files in the command line.
    kubectl create -f deployment.yml

The second way is with imperative commands. Here you define the objects using kubectl commands and flags.
    kubectl create deployment my-deployment --image=nginx      # Create a deployment named my-deployment with the nginx image.

QUICK SAMPLE YAML:
You can use "--dry-run" flag to run an imperative or declarative command without creating an object.
Combine it with "-o yaml" option on a imperative command to quickly obtain a sample YAML file you can manipulate.
By default it will write the YAML definition in STDOUT, you would need redirect to write it to a file.
    kubectl create deployment my-deployment --image=nginx --dry-run -o yaml    # Run the command without creating the object to create a sample YAML definition for the specified deployment.

RECORD A COMMAND:
You can use the "--record" flag to record the command that was used to make a change.
This will save the command in the object's annotations, and can be reviewed via the kubectl describe command.
    kubectl scale deployment my-deployment replicas=5 --record      # Record the command in the object annotations.


[MANAGING K8S ROLE-BASED ACCESS CONTROL (RBAC)]
Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization.
RBAC authorization uses the "rbac.authorization.k8s.io" API group to drive authorization decisions, allowing you to dynamically configure policies through the Kubernetes API.

In order to use K8s securely, you may need to exercise granular control over what users can do in your cluster, whether they are human users or automated processes.

Role-based access control (RBAC) in K8s allows you to control what users are allowed to do and access within your cluster.
For example, you can use RBAC to allow developers to read metadata and logs from Kubernetes pods but not make changes to them.

K8S RBAC OBJECTS:
You can create the objects in the cluster from a definition via the kubectl create or kubectl apply commands.

Roles & ClusterRoles:
Roles and ClusterRoles are Kubernetes objects that define a set of permissions. These permissions determine what users can do in the cluster.
The difference between them is that Roles defines permissions within a particular namespace, and ClusterRoles defines cluster-wide permissions not specific to a single namespace.
In the definition, Roles will have a namespace specified and ClusterRoles won't.

RoleBinding & ClusterRoleBinding:
RoleBinding and ClusterRoleBinding are objects that connect Roles and ClusterRoles to users.

EXAMPLE OF A ROLE DEFINITION:
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
    namespace: default
    name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "watch", "list"]

EXAMPLE OF A ROLE BINDING:
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
    name: pod-reader
    namespace: default
subjects:
- kind: User
  name: dev
  apiGroup: rbac.authorization.k8s.io
roleRef:
    kind: Role
    name: pod-reader
    apiGroup: rbac.authorization.k8s.io


[CREATING SERVICEACCOUNTS]
A service account provides an identity for processes that run in a Pod.
When you (a human) access the cluster (for example, using kubectl), you are authenticated by the apiserver as a particular User Account (currently this is usually "admin", unless your cluster admin has customized the cluster).
Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example "default").

RBAC is a powerful way to manage access to Kubernetes resources for regular user accounts.
However, you can also use it to manage access from within your Pods. Service Accounts allow you to control the ways in which Pods in the cluster can utilize the K8s API.

CREATING SERVICE ACCOUNTS:
A service account object can be created with some YAML just like any other K8s object.
Example:
---
apiVersion: v1
kind: ServiceAccount
metadata:
    name: my-serviceaccount

You can use the imperative commands to create Service Accounts to.
    kubectl create serviceaccount my-serviceaccount2 -n my-namespace    # Create my-serviceaccount2 SA in the specified namespace.

To check your service accounts you can use command:
    kubectl get sa          # Display a list of the existing Service Accounts in default Namespace.

To check in-depth information about a Service Account you can use command:
    kubectl describe sa my-serviceaccount       # Display more info for the Service Account.

BINDING ROLES TO SERVICE ACCOUNTS:
You can manage access control for service accounts, just like any other user, using RBAC objects.
Bind Service Accounts with ClusterRoles or ClusterRoleBindings to provide access to K8s API functionality.
Example:
...
subjects:
- kind: ServiceAccount
  name: my-serviceaccount
  namespace: default
...


[INSPECTING POD RESOURCE USAGE]
When you are running applications on Kubernetes, it may be necessary to take a closer look at the resources your pods are using.

You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and characteristics of the overall cluster.
Kubernetes provides detailed information about an application's resource usage at each of these levels.

In Kubernetes, application monitoring doesn't depend on a single monitoring solution.
On a new cluster, you can use "resource metrics" or "full metrics" pipelines to collect monitoring statistics.

RESOURCE METRICS PIPELINE:
The resource metrics pipeline provides a limited set of metrics related to the cluster components such as the Horizontal Pod Autoscaler controller, as well as the "kubectl top" utility.
These metrics are collected by the lightweight, short-term, in-memory "metrics-server" and are exposed via the "metrics.k8s.io" API.
"metrics-server" discovers all nodes on the cluster and queries each node's kubelet for CPU and memory usage.

FULL METRICS PIPELINE:
A full metrics pipeline gives you access to richer metrics.
Kubernetes can respond to these metrics by automatically scaling or adapting the cluster based on its current state, using mechanisms such as the Horizontal Pod Autoscaler.
The monitoring pipeline fetches metrics from the kubelet and then exposes them to Kubernetes via an adapter by implementing either the "custom.metrics.k8s.io" or "external.k8s.io" API.
Prometheus, a CNCF project, can natively monitor Kubernetes, nodes, and Prometheus itself.

KUBERNETES METRICS SERVER:
In order to view metrics about the resources pods and containers are using, you need an add-on to collect and provide that data.
One such add-on is "Kubernetes Metrics Server".

KUBECTL TOP:
With kubectl top command, you can view data about resource usage in your pods and nodes.
kubectl top also supports flags similar to the ones in kubectl get such as "--sort-by" and "--selector".
Structure:
    kubectl top pod [FLAGS]
    kubectl top nodes [FLAGS]
Flags:
    - --sort-by JSONPATH: Sort output using a JSONPath expression.
    - --selector SELECTOR: Filter results by label.
Example:
    kubectl top pod --sort-by cpu                   # Display resource utilization info from all pods in default namespace sorted by cpu usage.
    kubectl top pod --selector app=metrics-test     # Display resource utilization info from pods in default namespace that has the label specified.
    kubectl top pod PODNAME --containers            # Show metrics for a given pod and its containers.
    kubectl top nodes                               # Display resource utilization info from all cluster nodes.



----------------------------------------------- PODS & CONTAINERS -----------------------------------------------
[MANAGING APPLICATION CONFIGURATION]
When you are running applications in Kubernetes, you may want to pass dynamic values to your applications at runtime to control how they behave.
This is known as application configuration.

With Kubernetes, you have 2 options to store configuration data:
    - Using ConfigMaps.
    - Using Secrets.

CONFIGMAPS:
A ConfigMap is an API object used to store non confidential data in key-value pairs.
Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.

A ConfigMap allows you to decouple environment-specific configurations from your container images, so that your applications are easily portable.

Example:
apiVersion: v1
kind: ConfigMap
metadata:
    name: my-configmap
data:
    key1: value1
    key2: value2
    key3:
        subkey:
            morekeys: data
            evenmore: some more data
    key4: |
        You can also do
        multi-line
        data.

NOTE: ConfigMap doesn't provide secrecy or encryption.
If the data you want to store are confidential, use a Secret rather than a ConfigMap, or use additional (third party) tools to keep your data private.

SECRETS:
Kubernetes Secrets let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys.
Storing confidential information in a Secret is safer and more flexible than putting it verbatim in a Pod definition or in a container image.
They are created and used similarly to ConfigMaps.

Example:
apiVersion: v1
kind: Secret
metadata:
    name: my-secret
type: Opaque
data:
    username: myuserinbase64    # Values must be specified with their base64 encoding version, not the plain text when using YAML files.
    password: mypassinbase64    # Values must be specified with their base64 encoding version, not the plain text when using YAML files.

NOTE: Kubernetes Secrets are, by default, stored as unencrypted base64-encoded strings.
By default they can be retrieved (as plain text) by anyone with API access, or anyone with access to Kubernetes' underlying data store, etcd.
In order to safely use Secrets, it's recommended you (at least):
    1.  Enable Encryption at Rest for Secrets.
    2.  Enable or configure RBAC rules that restricts reading and writing the Secret.
        Be aware that secrets can be obtained implicitly by anyone with the permission to create a Pod.

ENVIRONMENT VARIABLES:
You can pass ConfigMap and Secret data to your containers as "environment variables".
These variables will be visible to your container process at runtime.

Example:
...
spec:
    containers:
    - ...
      env:
      - name: ENVVAR
        valueFrom:
            configMapKeyRef:
                name: my-configmap
                key: mykey
...

CONFIGURATION VOLUMES:
Configuration data from ConfigMaps and Secrets can also be passed to containers in the form of "mounted volumes".
This will cause the configuration data to appear in files available to the container file system.
Each top-level key in the configuration data will appear as a file containing all keys below that top-level key.

Example:
...
volumes:
- name: secret-vol
  secret:
    secretName: my-secret


[MANAGING CONTAINER RESOURCES]
When you specify a Pod, you can optionally specify how much of each resource a Container needs.
The most common resources to specify are CPU and memory (RAM). Although, they are others.

When you specify the resource request for Containers in a Pod, the scheduler uses this information to decide which node to place the Pod on.
When you specify a resource limit for a Container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.
The kubelet also reserves at least the request amount of that system resource specifically for that container to use.

RESOURCE REQUESTS:
Resource requests allow you to define an amount of resources (such as CPU or memory) you expect a container to use.
The Kubernetes scheduler will use resource requests to avoid scheduling pods on nodes that don't have enough available resources.
Containers are allowed to use more (or less) than the requested resources. Resource request only affect scheduling.

Memory is measured in bytes. CPU is measured in CPU units, which are 1/1000 of one CPU.

When creating a Pod with a resource request bigger than the resources in any node, the pod will be in "Pending" state, waiting until a node matches with the required resources.
It may be in Pending state indefinitely if a node doesn't have the resources.

Example:
apiVersion: v1
kind: Pod
metadata:
    name: my-pod-resource-request
spec:
    containers:
    - name: busybox
      image: busybox
      resources:
          requests:
              cpu: "250m"     # 1/4 of CPU.
              memory: "128Mi" # 128MB.

RESOURCE LIMITS:
Resource limits provide a way for you to limit the amount of resources your containers can use.
The container runtime is responsible for enforcing these limits, and different container runtimes do this differently.
Some runtimes will enforce these limits by terminating container processes that attempt to use more than the allowed amount of resources.

Example:
apiVersion: v1
kind: Pod
metadata:
    name: my-pod-resource-limit
spec:
    containers:
    - name: busybox
      image: busybox
      resources:
          limits:
              cpu: "250m"
              memory: "128Mi"


[MONITORING CONTAINER HEALTH WITH PROBES]
Kubernetes comes with a useful set of features for constantly monitoring the health and state of your containers.
With a little customization, you can use these features to create rather robust self-healing applications.

CONTAINER HEALTH:
K8s provides a number of features that allow you to build robust solutions, such as the ability to automatically restart unhealthy containers.
To make the most of these features, K8s needs to be able to accurately determine the status of your applications.
This means actively monitoring "container health".

CONTAINER PROBES:
A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the container.
There are 3 types of handlers:
    - ExecAction: Executes a specified command inside the container. The diagnostic is considered successful if the command exits with a status code of 0.
    - TCPSocketAction: Performs a TCP check against the Pod's IP address on a specified port. The diagnostic is considered successful if the port is open.
    - HTTPGetAction:    Performs an HTTP "GET" request against the Pod's IP address on a specified port and path.
                        The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400.

Each probe has one of three results:
    - Success: The container passed the diagnostic.
    - Failure: The container failed the diagnostic.
    - Unknown: The diagnostic failed, so no action should be taken.

The kubelet can optionally perform and react to 3 kinds of probes on running containers:
    - livenessProbe.
    - readinessProbe.
    - startupProbe.

LIVENESS PROBES:
Liveness probes allow you to automatically determine whether or not a container application is in healthy state.
By default, K8s will only consider a container to be "down" if the container process stops.
Liveness probes allow you to customize this detection mechanism and make it more sophisticated.
If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy.
If a container doesn't provide a liveness probe, the default state is "Success".

## Refer the liveness-probe-exec.yml & liveness-probe-httpGet.yml files under examples/probes/ for an example of this.

READINESS PROBES:
Readiness probes are used to determine when a container is ready to accept requests.
When you have a service backed by multiple container endpoints, user traffic will not be sent to a particular pod until its containers have all passed the readiness checks defined by their readiness probes.
Use readiness probes to prevent user traffic from being sent to pods that are still in the process of starting up.
If the readiness probe fails, the endpoints controller removes the Pod's IP address from the endpoints of all Services that match the Pod.
The default state of readiness before the initial delay is "Failure". If a container doesn't provide a readiness probe, the default state is "Success".

## Refer the examples/probes/readiness-probe.yml file for an example of this.

STARTUP PROBES:
Startup probes are very similar to liveness probes.
However, while liveness probes run constantly on a schedule, startup probes run at container startup and stop running once they succeed.
They are used to determine when the application has successfully started up.
Startup probes are especially useful for legacy applications that can have long startup times.
All other probes are disabled if a startup probe is provided, until it succeeds.
If the startup probe fails, the kubelet kills the container, and the container is subjected to its restart policy.
If a container doesn't provide a startup probe, the default state is "Success".

## Refer the examples/probes/startup-probe.yml file for an example for this.


[BUILDING SELF-HEALING PODS WITH RESTART POLICIES]
Kubernetes restart policies instruct the Kubernetes system on how to handle unhealthy pods.
When combined with probes, restart policies can allow Kubernetes to automatically detect pod failures and take corrective action.

K8s can automatically restart containers when they fail.
"Restart policies" allow you to customize this behavior by defining when you want a pod's containers to be automatically restarted.
Restart policies are an important component of self-healing applications, which are automatically repaired when a problem arises.

The "spec" of a Pod has a "restartPolicy" field with possible values:
    - Always.
    - OnFailure.
    - Never.
The default is "Always".

The "restartPolicy" applies to all containers in the Pod. "restartPolicy" only refers to restarts of the containers by the kubelet on the same node.
After containers in a Pod exit, the kubelet restarts them an exponential back-off delay (10s, 20s, 40s, ...), that is capped at five minutes.
Once a container has executed for 10 minutes without any problems, the kubelet resets the restart backoff timer for that container.

RESTART POLICY 'Always':
Always is the default restart policy in K8s. With this policy, containers will always be restarted if they stop, even if they completed successfully.
Use this policy for applications that should always be running.

RESTART POLICY 'OnFailure':
The OnFailure restart policy will restart containers only if the container process exits with an error code or the container is determined to be unhealthy by a liveness probe.
Use this policy for applications that need to run successfully and then stop.

RESTART POLICY 'Never':
The Never restart policy will cause the pod's containers to never be restarted, even if the container exits or a liveness probe fails.
Use this for applications that should run once and never be automatically restarted.

## Refer the files under examples/restart-policies for examples of this topic.


[CREATING MULTI-CONTAINER PODS]
Pods in Kubernetes cluster are used in 2 main ways:
    - Pods that run a single container: The "one-container-per-Pod" model is the most common Kubernetes use case. In this case, you can think of a Pod as a wrapper around a single container.
                                        Kubernetes manages Pods rather than managing the containers directly.
    - Pods that run multiple containers that need to work together: A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit of service.
                                                                    For example, one container serving data stored in a shared volume to the public, while a separate "sidecar" container refreshes or updates those files.
                                                                    The Pod wraps these containers, storage resources, and ephemeral network identity together as a single unit.

NOTE: Grouping multiple co-located and co-managed containers in a single Pod is a relatively advanced use case. You should use this pattern only in specific instances in which your containers are tightly coupled.

MULTI-CONTAINER POD:
A Kubernetes Pod can have one or more containers. A Pod with more than one container is a multi-container Pod.
In a multi-container Pod, the containers share resources such as network and storage. They can interact with one another, working together to provide functionality.
As best practice, you should keep containers in separate Pods unless they need to share resources.

CROSS-CONTAINER INTERACTION:
Containers sharing the same Pod can interact with one another using shared resources:
    - Network: Containers share the same networking namespace and can communicate with one another on any port, even if that port is not exposed to the cluster.
    - Storage: Containers can use volumes to share data in a Pod.

EXAMPLE USE CASE OF MULTI-CONTAINER POD:
You have an application that is hard-coded to write log output to a file on disk.
You add a secondary container to the Pod (sometimes called a "sidecar") that reads the log file from a shared volume and prints it to the console so the log output will appear in the container log.

## Refer the pod definitions under examples/multi-container-pods/ for examples on this.


[INTRODUCTION TO INIT CONTAINERS]
A Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started.
Init containers are containers that run once during the startup process of a pod.
Init containers are exactly like regular container, except:
    - Init containers always run to completion.
    - Each init container must complete successfully before the next one starts.

If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds. However, if the Pod has a "restartPolicy" of "Never", and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.
To specify an init container for a Pod, add the "initContainers" field into the Pod specification, as an array of objects of type "Container", alongside the app "containers" array.

DIFFERENCES FROM REGULAR CONTAINERS:
Init containers support all the fields and features of app containers, including resource limits, volumes, and security settings.
However, the resource requests and limits for an init container are handled differently.

Also, init containers don't support "lifecycle", "livenessProbe", readinessProbe", or "startupProbe" because they must run to completion before the Pod can be ready.

If you specify multiple init containers for a Pod, kubelet runs each init container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, kubelet initializes the application containers for the Pod and runs them as usual.

You can use init containers to perform a variety of startup tasks. They can contain and use software and setup scripts that are not needed by your main containers.
They are often useful in keeping your main containers lighter and more secure by offloading startup tasks to a separate container.

USE CASES:
    - Cause a Pod to wait for another K8s resource to be created before finishing startup.
    - Perform sensitive startup steps securely outside of app containers.
    - Populate data into a shared volume at startup.
    - Communicate with another service at startup.

## Refer the file examples/init-container.yml for an example of this.



----------------------------------------------- ADVANCED POD ALLOCATION -----------------------------------------------
[EXPLORING K8S SCHEDULING]
In Kubernetes, "scheduling" refers to making sure that Pods are matched to Nodes so that Kubelet can run them.

SCHEDULER:
A scheduler watches for newly created Pods that have no Node assigned.
For every Pod that the scheduler discovers, the scheduler becomes responsible for finding the best Node for that Pod to run on.
"kube-scheduler" is the default scheduler for Kubernetes and runs as part of the control plane.

SCHEDULING PROCESS:
In a cluster, Nodes that meet the scheduling requirements for a Pod are called "feasible" nodes.
If none of the nodes are suitable, the pod remains unscheduled until the scheduler is able to place it.

The scheduler finds feasible Nodes for a Pod and then runs a set of functions to score the feasible Nodes and picks a Node with the highest score among the feasible ones to run the Pod.
The scheduler then notifies the API server about this decision in a process called "binding".

The Kubernetes scheduler selects a suitable Node for each Pod. It takes into account things like:
    - Resource requests vs. available node resources.
    - Various configurations that affect scheduling using node labels.

ASSIGNING PODS TO NODES:
You can constrain a Pod to only be able to run on particular Node(s), or to prefer to run on particular Nodes.
There are several ways to do this, and the recommended approaches all use "label selectors" to make the selection.

NODESELECTOR:
nodeSelector is the simplest recommended form of node selection constraint. nodeSelector is a field of PodSpec. It specifies a map of key-value pairs.
For the Pod to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels (it can have additional labels as well).

Example:
apiVersion: v1
kind: Pod
metadata:
    name: nginx-nodeselector
spec:
    containers:
    - name: nginx
      image: nginx
    nodeSelector:
        mylabel: myvalue

NODENAME:
You can bypass scheduling and assign a Pod to a specific Node by name using nodeName. nodeName is a field of PodSpec.
Some of the limitations of using nodeName to select nodes are:
    - If the named node doesn't exists, the pod won't run, and in some cases may be automatically deleted.
    - If the named node doesn't have the resources to accommodate the Pod, the Pod will fails and its reason will indicate why, for example OutOfmemory or OutofCpu.
    - Node names in cloud environments are not always predictable or stable.

Example:
apiVersion: v1
kind: Pod
metadata:
    name: nginx-nodename
spec:
    containers:
    - name: nginx
      image: nginx
    nodeName: k8s-worker1

NODE LABELING:
You can assign labels to your cluster's nodes using the "kubectl label" command.
    kubectl label node NODENAME special=true    # Assigns the label "special: true" to the specified node.


[USING DAEMONSETS]
A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them.
As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.

Some typical uses of a DaemonSet are:
    - Running a cluster storage daemon on every node.
    - Running a logs collection daemon on every node.
    - Running a node monitoring daemon on every node.

In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.
A more complex setup might use multiple DaemonSets for a single type of daemon, but with different flags and/or different memory and cpu requests for different hardware types.

DaemonSets are part of the "apps/v1" API.

DAEMONSETS AND SCHEDULING:
DaemonSets respect normal scheduling rules around node labels, taints, and tolerations.
If a Pod wouldn't normally be scheduled on a node, a DaemonSet won't create a copy of the Pod on that node.


[USING STATIC PODS]
The Kubernetes scheduler and DaemonSets provide different ways for the Kubernetes control plane to allocate pods.
However, individual kubelets can also allocate pods on their own node using "Static Pods".

"Static Pods" are managed directly by the kubelet daemon on a specific node, without the API server observing them.
Unlike Pods that are managed by the control plane (for example, a Deployment); instead, the kubelet watches each static Pod (and restarts it if it fails).

Static Pods are always bound to one Kubelet on a specific node.

Kubelet automatically creates static Pods from YAML manifest files located in the manifest path on the node.
This manifest path is configurable. The default location set up by kubeadm is "/etc/kubernetes/manifests/".

Kubelet regularly check this directory and creates Static Pods if a YAML manifest file is present.
You can force kubelet to immediately create the Static Pod by restarting the kubelet service.
    sudo systemctl restart kubelet

The content of the YAML manifest files for Static Pods specification is just the same as regular Pods. The only difference is the place they reside.

MIRROR PODS:
The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod.
This means that the Pods running on a node are visible on the API server, but cannot be controlled from there.
The Pod names will suffixed with the node hostname with a leading hyphen.

NOTE: If you are running clustered Kubernetes and are using static Pods to run a Pod on every node, you should probably be using a DaemonSet instead.



----------------------------------------------- DEPLOYMENTS -----------------------------------------------
[DEPLOYMENTS OVERVIEW]
A "Deployment" is a K8s object that provides declarative updates for Pods and ReplicaSets (a ReplicaSet is a set of replica Pods). It defines the desired state for a ReplicaSet.
You describe a desired state in a Deployment, and the "Deployment Controller" changes the actual state to the desired state at a controlled rate.
You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.

A Deployment's desired state includes:
    - replicas: The number of replica Pods the Deployment will seek to maintain.
    - selector: A label selector used to identify the replica Pods managed by the Deployment.
    - template: A template Pod definition used to create replica Pods.

Deployments are part of the "apps/v1" API.

USE CASE:
The following are typical use cases for Deployments.
    - Create a Deployment to rollout a ReplicaSet.
    - Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment (perform rolling updates).
    - Rollback to an earlier Deployment revision if the current state of the Deployment is not stable.
    - Scale up the Deployment to facilitate more load (or scale down).
    - Pause the Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.
    - Use the status of the Deployment as an indicator that a rollout has stuck.
    - Clean up older ReplicaSet that you don't need anymore.


[SCALING APPLICATIONS WITH DEPLOYMENTS]
One of the use cases for deployments is application scaling.

SCALING:
Scaling refers to dedicating more (or fewer) resources to an application in order to meet changing needs.
K8s Deployments are very useful in "horizontal scaling", which involves changing the number of containers running an application.

DEPLOYMENT SCALING:
The Deployment's "replicas" setting determines how many replicas are desired in its desired state.
If the "replicas" number is changed, replica Pods will be created or deleted to satisfy the new number.

HOW TO SCALE A DEPLOYMENT:
You can scale a deployment simply by changing the number of "replicas" in the YAML descriptor with "kubectl apply" command.
Or by using "kubectl edit" to edit the deployment's YAML descriptor in place.
Example:
...
spec:
    replicas: 5
...

The other way is by using the special "kubectl scale" command. This command is specifically to scale deployments up and down.
    kubectl scale deployment.v1.apps/my-deployment --replicas 5     # Scale "my-deployment" to have 5 replicas.


[MANAGING ROLLING UPDATES WITH DEPLOYMENTS]
ROLLING UDPATE:
Rolling updates allow you to make changes to a Deployment's Pods at controlled rate, gradually replacing old Pods with new Pods.
This allows you to update your Pods without incurring downtime.

ROLLBACK:
If an update to a deployment causes a problem, you can rollback the deployment to a previous working state.

PERFORMING A ROLLOUT:
To trigger a rollout, it's the same as for scaling a deployment.
You can simply modify the YAML descriptor of your deployment and use the "kubectl apply" command. Or by using the "kubectl edit" command.
However, a rollout will only be performed if there is a change in the Deployment's Pod template.

You can also use the "kubectl set image" command to perform a rollout.
    kubectl set image deployment/DEPLOYMENT CONTAINER=IMAGE
    kubectl set image deployment/my-deployment nginx=nginx:latest   # Perform a rollout by updating the image used in the "nginx" container on the "my-deployment" deployment.

To display the status of the last rolling update, you can run the command:
    kubectl rollout status deployment.v1.apps/my-deployment     # Display the status of the last rollout of "my-deployment".

To display the history of the rollouts in a deployment, you can use the "kubectl rollout history" command.
If you add the "--record" flag at the moment of the rollout, you will find the command that cased the change in the history notations.
    kubectl rollout history deployment.v1.apps/my-deployment    # Display the rollout history of "my-deployment".

To perform a rollback by undoing the last rollout, you can use the next command:
    kubectl rollout undo deployment.v1.apps/DEPLOYMENT
    kubectl rollout undo deployment.v1.apps/my-deployment       # Rollback the last rollout.
To rollout to an specific revision version, you can use the "--to-revision" flag
    kubectl rollout undo deployment.v1.apps/my-deployment --to-revision=1   # Rollback the deployment back to revision 1.

NOTE: A Deployment's revision is created when a Deployment's rollout is triggered.
This means that the new revision is created if and only if the Deployment's Pod template (.spec.template) is changed, for example if you update the labels for container images of the template.
Other updates, such as scaling the Deployment, don't create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.
This means that when you roll an update, or roll back to an earlier revision, only the Deployment's Pod template part triggers a rollout.



----------------------------------------------- NETWORKING -----------------------------------------------
[K8S NETWORKING ARCHITECTUAL OVERVIEW]
KUBERNETES NETWORK MODE:
The Kubernetes network model is a set of standards that define how networking between Pods behaves.
There are a variety of different implementations of this model, including the Calico network plugin.

NETWORK MODEL ARCHITECTURE:
The Kubernetes network model defines how Pods communicate with each other, regardless of which Node they are running on.
Each Pod has its own unique IP address within the cluster. Any Pod can reach any other Pod using the Pod's IP address.
This creates a virtual network that allows Pods to easily communicate with each other, regardless of which node they are on.

Kubernetes imposes the following fundamental requirements on any networking implementation (barring any intentional network segmentation policies):
    - Pods on a node can communicate with all Pods on all notes without NAT.
    - Agents on a node (e.g., system daemons, kubelet) can communicate with all pods on that node.

NOTE: For platforms that support "Pods" running in the host network (e.g. Linux):
    - Pods in the host network of a node can communicate with all Pods on all nodes without NAT.

This model is not only less complex overall, but it's principally compatible with the desire for Kubernetes to enable low-friction porting of apps from VMs to containers.
If your job previously ran in a VM, your VM had an IP and could talk to other VMs in your project. This is the same basic model.

Kubernetes IP addresses exist at the Pod scope. Containers within a Pod share their same network namespace, including their IP address and MAC address.
This means that containers within a Pod can all reach each other's ports on "localhost".
This also means that containers within a Pod must coordinate port usage, but this is no different from processes in a VM. This is called the "IP-per-Pod" model.

It's possible to request ports on the "Node" itself which forward to your Pod (called host ports), but this is a very niche operation.


[CNI PLUGINS OVERVIEW]
Network plugins in Kubernetes come in a few flavors:
    - CNI plugins:  Adhere to the Container Network Interface (CNI) specification designed for interoperability.
                    Kubernetes follows the v0.4.0 release of the CNI specification.
    - Kubenet plugin: Implements basic "cbr0" using the "bridge" and "host-local" CNI plugins.

CNI plugins are a type of Kubernetes network plugin.
These plugins provide network connectivity between Pods according to the standard set by the Kubernetes network model. There are many CNI network plugins.

SELECTING A NETWORK PLUGIN:
Which network plugin is best for you will depend on your specific situation. Check the Kubernetes documentation for a list of available plugins.
You may need to research some of these plugins for yourself, depending on your production use case. Some examples of network plugins are:
    - Calico.
    - Flannel.

INSTALLING NETWORK PLUGINS:
Each plugin has its own unique installation process.

NOTE: Kubernetes nodes will remain "NotReady" until a network plugin is installed. You will be unable to run Pods while this is the case.


[UNDERSTANDING K8S DNS]
Kubernetes creates DNS records for Services and Pods. You can contact services with consistent DNS names instead of IP addresses.

Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures the kubelet to tell individual containers to use the DNS Service's IP to resolve DNS names.
Every Service defined in the cluster (including the DNS server itself) is assigned a DNS name.
By default, a client Pod's DNS search list includes the Pod's own namespace and the cluster's default domain.

The DNS service can usually be found in the "kube-system" namespace.

For K8s clusters created via "kubeadm", the CoreDNS solution is used.

POD DOMAIN NAMES:
All Pods in a "kubeadm" cluster are automatically given a domain name of the following form:
    POD-IP-ADDRESS.NAMESPACENAME.pod.cluster.local
In the Pod IP address, the dots are replaces by dashes.

Example:
A Pod in the "default" namespace with the IP address "192.168.10.100" would have a domain name as:
    192-168-10-100.default.pod.cluster.local

Since the Pod IP is part of the Pod's DNS name is not that useful. DNS name are really useful when used with Services.


[USING NETWORK POLICIES]
The Kubernetes cluster network makes it very easy to establish network communication between Pods.
However, this can present a challenge when it comes to securing your cluster. NetworkPolicies allow you to control network traffic into and out of your Pods.
They are a great way to make your Kubernetes applications and infrastructure more secure.

A K8s NetworkPolicy is an object that allows you to control the flow of network communication (OSI layer 3 & 4) to and from Pods.
This allows you to build a more secure cluster network by keeping Pods isolated from traffic they don't need.

The entities that a Pod can communicate with are identified through a combination of the following 3 identifiers:
    - Other Pods that are allowed (exception: a Pod cannot block access to itself).
    - Namespaces that are allowed.
    - IP blocks (exception: Traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the Pod or the Node).
When defining a Pod- or Namespace-based NetworkPolicy, you use a "selector" to specify what traffic is allowed to and from the Pod(s) that match that selector.
Meanwhile, when IP based NetworkPolicies are created, you define policies based on IP blocks (CIDR ranges).

NOTE: Network policies are implemented by the "network plugin". To use network policies, you must be using a networking solution which supports NetworkPolicy.
Creating a NetworkPolicy resource without a controller that implements it will have no effect.

NETWORK POLICIES' COMPONENTS - podSelector:
A podSelector determines to which Pods in the namespace the NetworkPolicy applies. The podSelector can select Pods using Pod labels.
By default, Pods are considered non-isolated and completely open to all communication.
If any NetworkPolicy selects a Pod, the Pod is considered isolated and will only be open to traffic allowed by NetworkPolicies.

Example:
apiVersion:networking.k8s.io/v1
kind: NetworkPolicy
metadata:
    name: my-network-policy
spec:
    podSelector:
        matchLabels:
            role: db

NETWORK POLICIES' COMPONENTS - Ingress & Egress:
A NetworkPolicy can apply to Ingress, Egress, or both.

Ingress refers to the incoming network traffic coming into the Pod from another source.
Egress refers to the outgoing network traffic leaving the Pod for another destination.

NETWORK POLICIES' COMPONENTS - from AND to SELECTORS:
The "from" and "to" selector are related to Ingress and Egress. They determined which traffic is allowed by the NetworkPolicy.
The "from selector" selects ingress traffic that will be allowed.
The "to selector" selects egress traffic that will be allowed.

Example:
...
spec:
    ingress:
    - from:
      ...
    egress:
    - to:
      ...

from & to SELECTOR TYPES:
podSelector: Select Pods to allow traffic from/to.
Example:
...
spec:
    ingress:
    - from:
      - podSelector:
            matchLabels:
                app: db

namespaceSelector: Select namespaces to allow traffic from/to. You can use labels to select a namespace:
Example:
...
spec:
    ingress:
    - from:
      - namespaceSelector:
            matchLabels:
                app: db

ipBlock: Select an IP range to allow traffic from/to.
...
spec:
    ingress:
    - from:
      - ipBock:
            cidr: 172.17.0.0/16

PORTS:
"port" specifies one or more ports that will allow traffic.
Traffic is only allowed if it matches both an allowed port and one of the "from/to" rules.
Example:
...
spec:
    ingress:
    - from:
      ports:
      - protocol: TCP
        port: 80



----------------------------------------------- SERVICES -----------------------------------------------
[K8S SERVICES OVERVIEW]
In Kubernetes, a Service is an abstraction which defines a logical set of Pods and policy by which to access them (sometimes this pattern is called a micro-service).
The set of Pods targeted by a Service is usually determined by a "selector".

SERVICE ROUTING:
Clients make requests to a Service, which routes traffic to its Pods in a load-balanced fashion.

ENDPOINTS:
Endpoints are the backend entities to which Services route traffic. For a Service that routes traffic to multiple Pods, each Pod will have an endpoint associated with the Service.
One way to determine which Pod(s) a Service is routing traffic to is to look at that Service's Endpoints.


[USING K8S SERVICES]
You can have any number of services routing to the same Pods if needed.

SERVICE TYPES:
Each Service has a type. The Service type determines how and where the Service will expose your application. There are 4 service types:
    - ClusterIP.
    - NodePort.
    - LoadBalancer.
    - ExternalName.

ClusterIP SERVICES:
ClusterIP Services expose applications inside the cluster network. Use them when your clients will be other Pods within the cluster.

## Refer to the examples/services/clusterip-svc.yml file for an example of this.

NodePort SERVICES:
NodePort Services expose applications outside the cluster network. Use NodePort when applications or users will be accessing your application form outside the cluster.
In a NodePort Service, you specify the cluster node port in which the node will be listening for the service with the property "nodePort". If you don't specify this, Kubernetes will automatically assign one.
In a NodePort Service, all cluster nodes (including the control plane ones) will be listening in the specified port for the service.

## Refer to the examples/services/nodeport-svc.yml file for an example of this.

LoadBalancer SERVICES:
LoadBalancer Services also expose applications outside the cluster network, but they use an external cloud load balancer to do so.
This service type only work with cloud platforms that include load balancing functionality.

To display a Service's endpoints you can run the next command:
    kubectl get endpoints SERVICE
    kubectl get endpoints my-clusterip-svc   # Display the Service endpoints (Pod IPs).

To reference or hit the service you would need to specify only the service name.
    kubectl exec TESTINGPOD -- curl SERVICE:PORT
    kubectl exec svc-testing-pod -- curl my-cluster-ip-svc      # Hits a Pod via a service.


[DISCOVERING K8S SERVICES WITH DNS]
The Kubernetes DNS server doesn't just allow pods to locate one another directly, it also allows them to allocate services.

SERVICE DNS NAMES:
The Kubernetes DNS (Domain Name System) assign DNS names to Services, allowing applications within the cluster to easily locate them.
A service's fully qualified domain name has the following format:
    SERVICENAME.NAMESPACE.svc.CLUSTERDOMAINNAME
The default cluster domain name for a Kubernetes cluster is "cluster.local".
Example:
    my-clusterip-svc.default.svc.cluster.local

SERVICE DNS AND NAMESPACES:
A Service's fully qualified domain name can be used to reach the service from within any Namespace in the cluster.
    my-clusterip-svc.default.svc.cluster.local
However, Pods within the same Namespace can also simply use the service name.
    my-clusterip-svc


[MANAGING ACCESS FROM OUTSIDE WITH K8S INGRESS]
When running applications in Kubernetes, you will likely want outside entities to be able to interact with them.
Kubernetes Ingress objects allow you to define how this access occurs.

INGRESS:
An Ingress is a Kubernetes object that manages external access to Services in the cluster. It exposes HTTP & HTTPS routes from outside the cluster to services within the cluster.
An Ingress is capable of providing more functionality than a simple NodePort Service, such as SSL termination, advanced load balancing, or name-based virtual hosting.
Traffic routing is controlled by rules defined on the Ingress resource.

An Ingress doesn't expose arbitrary ports or protocols.
Exposing services other than HTTP and HTTPS to the internet typically uses a service of type "Service.Type=NodePort" or "Service.Type=LoadBalancer".

INGRESS CONTROLLER:
Ingress objects actually do nothing by themselves. In order for Ingresses to do anything, you must install one or more "Ingress controllers".
There are a variety of Ingress Controllers available, all of which implement different methods for providing external access to your Services.
Unlike other types of controllers which run as part of the "kube-controller-manager" binary, Ingress controllers are not started automatically with a cluster.

Kubernetes as a project supports and maintains "AWS", "GCE", and "nginx" ingress controllers.

ROUTING TO A SERVICE:
Ingresses define a set of "routing rules". A routing rule's properties determine to which requests it applies.
Each rule has a set of "paths", each with a "backend". Requests matching a path will be routed to its associated backend.

In this example, a request "http://SOMEENDPOINT/SOMEPATH" would be routed to port 80 on the "my-service" Service:
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
    name: my-ingress
spec:
    rules:
    - http:
        paths:
        - path: /somepath
          pathType: Prefix
          backend:
            service:
                name: my-service
                port:
                    number: 80

ROUTING TO A SERVICE WITH A NAMED PORT:
If a Service uses a "named port", an Ingress can also use the port's name to choose to which port it will route.

Example:
# Service definition:
apiVersion: v1
kind: Service
metadata:
    name: my-service
spec:
    selector:
        app: my-app
    ports:
    - name: web
      protocol: TCP
      port: 80
      targetPort: 8080
---
# Ingress definition:
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
    name: my-ingress
spec:
    rules:
    - http:
        paths:
        - path: /somepath
          pathType: Prefix
          backend:
            service:
                name: my-service
                port:
                    name: web



----------------------------------------------- STORAGE -----------------------------------------------
[K8S STORAGE OVERVIEW]
CONTAINER FILE SYSTEMS:
The container file system is "ephemeral". Files on the container's file system exist only as long as the container exists.
If a container is deleted or re-created in K8s, data stored on the container file system is lost.

VOLUMES:
Many applications need a more persistent method of data storage.
"Volumes" allow you to store data outside the container file system while allowing the container to access the data at runtime.
This can allow data to persist beyond the life of the container.

PERSISTENT VOLUMES:
Volumes offer a simple way to provide external storage to containers within the Pod/container spec. "Persistent Volumes" are a slightly more advanced form of a Volume.
They allow you to treat storage as an abstract resource and consume it using your Pods.

VOLUME TYPES:
Both Volumes and Persistent Volumes each have a "volume type". The volume type determines how the storage is actually handled.
Various volume types support storage methods such as:
    - NFS.
    - Cloud storage mechanisms (AWS, Azure, GCP).
    - ConfigMaps and Secrets.
    - A simple directory on the K8s node.


[USING K8S VOLUMES]
VOLUMES & VOLUMEMOUNTS:
Regular Volumes can be set up relatively easily within a Pod/container specification.

volumes:
In the Pod spec, these specify the storage volumes available to the Pod. They specify the volume type and other data that determines where and how the data is actually stored.
You can have any number of Volumes in a Pod.
Volumes are part of the Pod spec, not the container spec.

volumeMounts:
In the container spec, these reference the volumes in the Pod spec and provide a "mountPath" (the location on the file system where the container process will access the volume data).

Example:
apiVersion: v1
kind: Pod
metadata:
    name: volume-pod
spec:
    containers:
    - name: busybox
      image: busybox
      volumeMounts:
      - name: my-volume
        mountPath: /output
    volumes:
    - name: my-volume
      hostPath:
        path: /data

SHARING VOLUMES BETWEEN CONTAINERS:
You can use Volumes to share data between containers. You can use "volumeMounts" to mount the same volume to multiple containers within the same Pod.
This is a powerful way to have multiple containers interact with one another. For example, you could create a secondary sidecar container that processes or transforms output from another container.

Example:
apiVersion: v1
kind: Pod
metadata:
    name: volume-pod
spec:
    containers:
    - name: busybox1
      image: busybox
      volumeMounts:
      - name: my-volume
        mountPath: /output
    - name: busybox2
      image: busybox
      volumeMounts:
      - name: my-volume
        mountPath: /input
    volumes:
    - name: my-volume
      emptyDir: {}

COMMON VOLUME TYPES:
There are many volume types, but there are two you may want to be especially aware of:

hostPath:
Stores data in a specified directory on the K8s node.

emptyDir:
Stores data in a dynamically created location on the node. This directory exists only as long as the Pod exists on the node.
The directory and the data are deleted when the Pod is removed. This volume type is very useful for simply sharing data between containers in the same Pod.


[EXPLORING K8S PERSISTENT VOLUMES]
THE PersistentVolumes:
PersistentVolimes are K8s objects that allow you to treat storage as an abstract resource to be consumed by Pods, much like K8s treats compute resources such as memory and CPU.
A PersistentVolume uses a set of attributes to describe the underlying storage resource (such as a disk or cloud storage location) which will be used to store data.
Unlike with Volumes where you specify all the info in the Pod specification, PersistentVolumes allows you to specify all that information in a separate object.

reclaimPolicies:
A PersistentVolume's "persistentVolumeReclaimPolicy" determines how the storage resources can be reused when the PersistentVolume's associated PersistentVolumesClaims are deleted.
The values on this property are:
    - Retain: Keeps all data. This requires an administrator to manually clean up the data and prepare the storage resource for reuse.
    - Delete: Deletes the underlying storage resource automatically (only works for cloud storage resources).
    - Recycle: Automatically deletes all data in the underlying storage resource, allowing the PersistentVolume to be reused.

Example:
# The storageClassName attribute, is a reference to a Storage Class.
apiVersion: v1
kind: PersistentVolume
metadata:
    name: my-pv
spec:
    storageClassName: localdisk
    persistentVolumeReclaimPolicy: Recycle
    capacity:
        storage: 1Gi
    accessModes:
    - ReadWriteOnce
    hostPath:
        path: /var/output

STORAGE CLASSES:
Storage Classes allow K8s administrators to specify the types of storage services they offer on their platform.
For example, an administrator could create a StorageClass called "slow" to describe low-performance but inexpensive storage resources, and another called "fast" for high-performance but more costly resources.
This would allow users to choose storage resources that fit the needs of their applications.

Property allowVolumeExpansion:
The allowVolumeExpansion property of a StorageClass determines whether or not the StorageClass supports the ability to resize volumes after they are created.
If this property is not set to true, attempting to resize a volume that uses this StorageClass will result in an error.

Examples:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
    name: slow
provisioner: kubernetes.io/no-provisioner
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
    name: fast
provisioner: kubernetes.io/no-provisioner
allowVolumeExpansion: true

THE PersistentVolumeClaims:
A PersistentVolumeClaim represents a user's request for storage resources. It defines a set of attributes similar to those of a PersistentVolume (StorageClass, etc).
When a PersistentVolumeClaim is created, it will look for a PersistentVolume that is able to meet the requested criteria.
If it finds one, it will automatically be "bound" to the PersistentVolume.

Example:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: my-pvc
spec:
    storageClassName: localdisk
    accessModes:
    - ReadWriteOnce
    resources:
        requests:
            storage: 100Mi

USING A PersistentVolumeClaim IN A POD:
PersistentVolumeClaims can be mounted to a Pod's containers just like any other volume.
If the PersistentVolumeClaim is bound to a PersistentVolume, the containers will use the underlying PersistentVolume storage.

Example:
apiVersion: v1
kind: Pod
metadata:
    name: pv-pod
spec:
    containers:
    - name: busybox
      image: busybox
      volumeMounts:
      - name: pv-storage
        mountPath: /output
    volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: my-pvc

RESIZING A PersistentVolumeClaim:
You can expand PersistentVolumesClaims without interrupting applications that are using them.
Simply edit the "spec.resources.requests.storage" attribute of an existing PersistentVolumeClaim, increasing its value.
However, the StorageClass must support resizing volumes and must have "allowVolumeExpansion" set to true.

NOTE: accessModes needs to match between the PersistentVolume and the PersistentVolumeClaim.
A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.



----------------------------------------------- TROUBLESHOOTING -----------------------------------------------
[TROUBLESHOOTING YOUR K8S CLUSTER]
An important skill for any Kubernetes administrator is handling issues with the Kubernetes cluster itself.

KUBE API SERVER:
If the K8s API server is down, you will not be able to use kubectl to interact with the cluster.
You may get a message that looks something like:
    " The connection to the server localhost:6443 was refused - did you specify the right host or port?"
Assuming your kubeconfig is set up correctly, this may mean the API server is down.

Possible fixes:
Make sure your container runtime (such as Docker, containerd, etc) and kubelet services are up and running on your control plane node(s).

CHECKING NODE STATUS:
Check the status of your nodes to see if any of them are experiencing issues.
Use "kubectl get nodes" to see the overall status of each node.
    kubectl get nodes

Use "kubectl describe node" to get more information on any nodes that are not in the "READY" state.
    kubectl describe node NODENAME      # Display in-depth information about the node.

If a node is having problems, it may be because a service is down on that node.
Each node runs the "kubelet" and container runtime (i.e., Docker) services.

1. View the status of the service.
    systemctl status kubelet
2. Start a stopped service.
    systemctl start kubelet
3. Enable a service so it starts automatically on system startup.
    systemctl enable kubelet

CHECKING SYSTEM PODS:
In a "kubeadm" cluster, several K8s components run as pods in the "kube-system" namespace.
Check the status of these components with "kubectl get pods" and "kubectl describe pod".
    kubectl get pods -n kube-system
    kubectl describe pod PODNAME -n kube-system


[CHECKING CLUSTER & NODE LOGS]
Logs are crucial to any troubleshooting effort.

SERVICE LOGS:
You can check the logs from K8s-related services on each node using "journalctl".
    sudo journalctl -u kubelet
    sudo journalctl -u docker

CLUSTER COMPONENTS LOGS:
The Kubernetes cluster components have log output redirected to "/var/log". For example:
    /var/log/kube-apiserver.log             # Logs for the K8s API server (for clusters not created via kubeadm).
    /var/log/kube-scheduler.log             # Logs for the K8s Scheduler (for clusters not created via kubeadm).
    /var/log/kube-controller-manager.log    # Logs for the K8s controller manager (for clusters not created via kubeadm).
Note that these log files may not appear for kubeadm clusters, since some components run inside containers.
In that case, you can access them with "kubectl logs".


[TROUBLESHOOTING YOUR APPLICATIONS]
In addition to handling issues with the Kubernetes cluster itself, you will also need to be able to troubleshoot issues with Pods and containers running on your cluster.

CHECKING POD STATUS:
You can see a Pod's status with "kubectl get pods".
    kubectl get pods

Use "kubectl describe pod" to get more information about what may be going on with an unhealthy Pod.
    kubectl describe pod PODNAME

RUNNING COMMANDS INSIDE CONTAINERS:
If you need to troubleshoot what is going on inside a container, you can execute commands within the container with "kubectl exec".
    kubectl exec PODNAME -c CONTAINERNAME -- COMMAND
Note that you cannot use "kubectl exec" to run any software that is not present within the container.

Assuming a container has a shell installed on it, you can get an interactive shell to the container.
    kubectl exec trlbsht-pod -c busybox --stdin --tty -- /bin/sh    # Start an interactive shell in the container.
    kubectl exec trlbsht-pod -c busybox --it -- /bin/sh             # Start an interactive shell in the container.


[CHECKING CONTAINERS LOGS]
When troubleshooting issues with containers, it will likely become necessary to gain some visibility into what is happening inside a container.

CONTAINER LOGGING:
K8s containers maintain logs, which you can use to gain insight into what is going on within the container.
A container's log contains everything written to the standard output (STDOUT) and error (STDERR) streams by the container process.

COMMAND kubectl logs:
Use the kubectl logs command to view a container's logs.
    kubectl logs PODNAME -c CONTAINERNAME


[TROUBLESHOOTING K8S NETWORKING ISSUES]
In a Kubernetes cluster, networking issues can be troublesome to investigate.

kube-proxy AND DNS:
In addition to checking on your K8s networking plugin, it may be a good idea to look at kube-proxy and the K8s DNS if you are experiencing issues within the K8s cluster network.
In a kubeadm cluster, the K8s DNS and kube-proxy run as Pods in the kube-system namespace.

NETSHOOT:
You can run a container in the cluster that you can use to run commands to test and gather information about network functionality.
The "nicolaka/netshoot" image is a great tool for this. This image contains a variety of networking exploration and troubleshooting tools.
Create a container running this image, and then use "kubectl exec" to explore away.



----------------------------------------------- MISCELLANEOUS -----------------------------------------------
[COMMANDS]
NAMESPACES ##################################
kubectl get namespaces                      # Display K8s namespaces.
kubectl get pods --namespace my-namespace   # Display all pods in "my-namespace" namespace.
kubectl get pods --all-namespaces           # Display all the pods form all namespaces.
kubectl create namespace my-namespace       # Creates a namespace with the specified name.

SAFELY DRAINING A K8S NODE ##################
kubectl drain NODENAME                      # Drain the specified node form the cluster.
kubectl drain NODENAME --ignore-daemonsets  # Drain the specified node from the cluster ignoring any DaemonSet.
kubectl uncordon NODENAME                   # Uncordon the specified node to the cluster (re-active it to run containers again).

UPGRADING K8S WITH KUBE ADM #############################################################################
Control plane node upgrade ##############################################################################
sudo apt-get update && sudo apt-get install -y --allow-change-held-packages kubeadm=1.20.2-00           # Upgrade the hold kubeadm version.
kubeadm version                                                                                         # Display the kubeadm version.
kubectl drain CONTROLPLANENODE --ignore-daemonsets                                                      # Drain the specified control plane node ignoring daemonsets.
sudo kubeadm upgrade plan VERSION                                                                       # Plan the upgrade.
sudo kubeadm upgrade plan v1.20.2                                                                       # Plan the upgrade to version 1.20.2
sudo kubeadm upgrade apply VERSION                                                                      # Apply the upgrade.
sudo kubeadm upgrade apply v1.20.2                                                                      # Apply the upgrade to version 1.20.2
sudo apt-get update && sudo apt-get install -y --allow-change-held-packages {kubelet,kubectl}=1.20.2-00 # Upgrade the held kubectl & kubelet versions.
sudo systemctl daemon-reload                                                                            # Reload the unit files for the services.
sudo systemctl restart kubelet                                                                          # Restart kubelet service.
kubectl uncordon CONTROLPLANENODE                                                                       # Uncordon the control plane node specified.
kubectl get nodes                                                                                       # Display the cluster nodes.
Worker node upgrade #####################################################################################
kubectl drain WORKERNODE --ignore-daemonsets --force                                                    # On control plane node.
sudo apt-get update && sudo apt-get install -y --allow-change-held-packages kubeadm=1.20.2-00
kubeadm version
sudo kubeadm upgrade node                                                                               # Upgrade the kubelet configuration on the worker node.
sudo apt-get update && sudo apt-get install -y --allow-change-held-packages {kubelet,kubectl}=1.20.2-00
sudo systemctl daemon-reload
sudo systemctl restart kubelet
kubectl uncordon WORKERNODE	                                                                            # On control plane node.

BACKING UP AND RESTORING ETCD CLUSTER DATA ##############################
ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save SNAPSHOTNAME  # Takes an etcd snapshot with the name specified.
ETCDCTL_API=3 etcdctl --write-out=table snapshot status SNAPSHOTNAME    # Verify the snapshot status.
ETCDCTL_API=3 etcdctl snapshot restore FILENAME                         # Restore etcd data from given backup.

WORKING WITH KUBECTL ############################################
kubectl api-resources                                           # Display all the resource types available.
kubectl get pods                                                # Display pods in the default namespace.
kubectl get po                                                  # Display pods in the default namespace.
kubectl get pod my-pod -o wide                                  # Display pods matching the name with additional information.
kubectl get pods -o json                                        # Display pods in the default namespace in JSON format.
kubectl get pods -o yaml                                        # Display pods in the default namespace in YAML format.
kubectl get pods -o wide --sort-by .spec.nodeName               # Sorts by node name all of the pods in the default namespace.
kubectl get pods -n kube-system --selector k8s-app=calico-node  # Display only the pods in kube-system namespace with the specified label.
kubectl describe pod my-pod                                     # Display in-depth information about the pod.
kubectl create -f pod.yml                                       # Create the K8s object with the specifications in the file.
kubectl apply -f pod.yml                                        # If not created, create the K8s object with the specifications in the file. If no change is needed, it won't perform anything.
kubectl delete pod my-pod                                       # Delete the pod.
kubectl exec my-pod -c busybox -- echo "Hello world!"           # Execute the command in the container inside the pod.
kubectl logs PODNAME -c CONTAINERNAME                           # Get the logs of the container specified running inside the Pod specified.

KUBECTL TIPS ############################################################
kubectl create deployment my-deployment --image=nginx                   # Create a deployment named my-deployment with the nginx image.
kubectl create deployment my-deployment --image=nginx --dry-run -o yaml # Run the command without creating the object to create a sample YAML definition for the specified deployment.
kubectl scale deployment my-deployment replicas=5 --record              # Record the command in the object annotations.

SERVICE ACCOUNTS ####################################################
kubectl create serviceaccount my-serviceaccount2 -n my-namespace    # Create my-serviceaccount2 SA in the specified namespace.
kubectl get sa                                                      # Display a list of the existing Service Accounts in default Namespace.
kubectl describe sa my-serviceaccount                               # Display more info for the Service Account.

INSPECTING POD RESOURCE USAGE ###################
kubectl top pod --sort-by cpu                   # Display resource utilization info from all pods in default namespace sorted by cpu usage.
kubectl top pod --selector app=metrics-test     # Display resource utilization info from pods in default namespace that has the label specified.
kubectl top pod PODNAME --containers            # Show metrics for a given pod and its containers.
kubectl top nodes                               # Display resource utilization info from all cluster nodes.

EXPLORING K8S SCHEDULING ####################
kubectl label node NODENAME special=true    # Assigns the label "special: true" to the specified node.

SCALING APPLICATIONS WITH DEPLOYMENTS ###########################
kubectl scale deployment.v1.apps/my-deployment --replicas 5     # Scale "my-deployment" to have 5 replicas.

MANAGING ROLLING UDPATES WITH DEPLOYMENTS ###############################
kubectl set image deployment/my-deployment nginx=nginx:latest           # Perform a rollout by updating the image used in the "nginx" container on the "my-deployment" deployment.
kubectl rollout status deployment.v1.apps/my-deployment                 # Display the status of the last rollout of "my-deployment".
kubectl rollout history deployment.v1.apps/my-deployment                # Display the rollout history of "my-deployment".
kubectl rollout undo deployment.v1.apps/my-deployment                   # Rollback the last rollout.
kubectl rollout undo deployment.v1.apps/my-deployment --to-revision=1   # Rollback the deployment back to revision 1.

USING K8S SERVICES ##########################################
kubectl get endpoints my-clusterip-svc                      # Display the Service endpoints (Pod IPs).
kubectl exec svc-testing-pod -- curl my-cluster-ip-svc      # Hits a Pod via a service.

USING K8S PERSISTENT VOLUMES ####
kubectl get sc                  # Display the list of created StorageClasses.
kubectl get pv                  # Display the list of created PersistentVolumes.
kubectl get pvc                 # Display the list of created PersistentVolumeClaims.

TROUBLESHOOTING YOUR APPLICATIONS ###############################
kubectl exec trlbsht-pod -c busybox --stdin --tty -- /bin/sh    # Start an interactive shell in the container.
kubectl exec trlbsht-pod -c busybox --it -- /bin/sh             # Start an interactive shell in the container.


[FILES & DIRECTORIES]
FILES:
$HOME/.kube/config              # Default Kubernetes user-scoped configuration file.

DIRECTORIES:
$HOME/.kube                     # Kubernetes user-scoped config directory.
/etc/kubernetes/manifests/      # Default manifest path (for kubeadm) on a worker node for Static Pods.

[MISCELLANEOUS]
ENVIRONMENT VARIABLES:
KUBECONFIG                      # Variable that defines the Kubernetes config file.
